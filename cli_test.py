#!/usr/bin/env python3
"""
Crawl4AI CLI å‘½ä»¤åˆ—æ¸¬è©¦ç¯„ä¾‹
"""

import asyncio
import json
from crawl4ai import AsyncWebCrawler
from crawl4ai.chunking_strategy import RegexChunking
from crawl4ai.extraction_strategy import JsonCssExtractionStrategy

async def json_extraction_test():
    """JSON æ ¼å¼æ“·å–æ¸¬è©¦"""
    print("ğŸ“‹ é–‹å§‹ JSON æ“·å–æ¸¬è©¦...")
    
    # å®šç¾©è¦æ“·å–çš„è³‡æ–™çµæ§‹
    schema = {
        "name": "æ–°èæ–‡ç« ",
        "baseSelector": "article, .article, .post",
        "fields": [
            {
                "name": "title",
                "selector": "h1, h2, .title",
                "type": "text"
            },
            {
                "name": "content", 
                "selector": "p, .content",
                "type": "text"
            },
            {
                "name": "author",
                "selector": ".author, .by-author",
                "type": "text"
            },
            {
                "name": "date",
                "selector": ".date, .published",
                "type": "text"
            }
        ]
    }
    
    extraction_strategy = JsonCssExtractionStrategy(schema)
    
    async with AsyncWebCrawler() as crawler:
        result = await crawler.arun(
            url="https://news.ycombinator.com",
            extraction_strategy=extraction_strategy
        )
        
        print(f"âœ… JSON æ“·å–å®Œæˆ")
        if result.extracted_content:
            try:
                data = json.loads(result.extracted_content)
                print(f"ğŸ“Š æ“·å–åˆ° {len(data)} ç­†è³‡æ–™")
                if data:
                    print(f"ğŸ“ ç¬¬ä¸€ç­†è³‡æ–™ç¯„ä¾‹:")
                    print(json.dumps(data[0], indent=2, ensure_ascii=False))
            except json.JSONDecodeError:
                print(f"ğŸ“„ åŸå§‹æ“·å–å…§å®¹: {result.extracted_content[:500]}...")

async def regex_chunking_test():
    """æ­£è¦è¡¨é”å¼åˆ†å¡Šæ¸¬è©¦"""
    print("\nâœ‚ï¸ é–‹å§‹æ­£è¦è¡¨é”å¼åˆ†å¡Šæ¸¬è©¦...")
    
    # ä½¿ç”¨æ¨™é¡Œä¾†åˆ†å¡Šå…§å®¹
    chunking_strategy = RegexChunking(
        patterns=[r'\n#{1,6}\s+(.+)', r'\n\*\*(.+?)\*\*']
    )
    
    async with AsyncWebCrawler() as crawler:
        result = await crawler.arun(
            url="https://docs.python.org/3/tutorial/",
            chunking_strategy=chunking_strategy
        )
        
        print(f"âœ… åˆ†å¡Šå®Œæˆ")
        if hasattr(result, 'chunks') and result.chunks:
            print(f"ğŸ“¦ åˆ†å¡Šæ•¸é‡: {len(result.chunks)}")
            for i, chunk in enumerate(result.chunks[:3], 1):
                preview = chunk[:100] + "..." if len(chunk) > 100 else chunk
                print(f"   ğŸ“ å¡Š {i}: {preview}")

async def links_analysis_test():
    """é€£çµåˆ†ææ¸¬è©¦"""
    print("\nğŸ”— é–‹å§‹é€£çµåˆ†ææ¸¬è©¦...")
    
    async with AsyncWebCrawler() as crawler:
        result = await crawler.arun(
            url="https://github.com/unclecode/crawl4ai"
        )
        
        print(f"âœ… é€£çµåˆ†æå®Œæˆ")
        
        if result.links:
            internal_links = result.links.get('internal', [])
            external_links = result.links.get('external', [])
            
            print(f"ğŸ  å…§éƒ¨é€£çµæ•¸é‡: {len(internal_links)}")
            print(f"ğŸŒ å¤–éƒ¨é€£çµæ•¸é‡: {len(external_links)}")
            
            if internal_links:
                print(f"ğŸ“‹ å‰5å€‹å…§éƒ¨é€£çµ:")
                for i, link in enumerate(internal_links[:5], 1):
                    print(f"   {i}. {link}")
            
            if external_links:
                print(f"ğŸ“‹ å‰5å€‹å¤–éƒ¨é€£çµ:")
                for i, link in enumerate(external_links[:5], 1):
                    print(f"   {i}. {link}")

async def metadata_extraction_test():
    """å…ƒè³‡æ–™æ“·å–æ¸¬è©¦"""
    print("\nğŸ“Š é–‹å§‹å…ƒè³‡æ–™æ“·å–æ¸¬è©¦...")
    
    async with AsyncWebCrawler() as crawler:
        result = await crawler.arun(
            url="https://www.python.org"
        )
        
        print(f"âœ… å…ƒè³‡æ–™æ“·å–å®Œæˆ")
        
        if result.metadata:
            print(f"ğŸ“‹ å…ƒè³‡æ–™:")
            for key, value in result.metadata.items():
                if isinstance(value, str) and len(value) > 100:
                    value = value[:100] + "..."
                print(f"   {key}: {value}")

async def performance_test():
    """æ•ˆèƒ½æ¸¬è©¦"""
    print("\nâš¡ é–‹å§‹æ•ˆèƒ½æ¸¬è©¦...")
    
    import time
    
    urls = [
        "https://example.com",
        "https://httpbin.org/html",
        "https://quotes.toscrape.com"
    ]
    
    start_time = time.time()
    
    async with AsyncWebCrawler() as crawler:
        for url in urls:
            single_start = time.time()
            result = await crawler.arun(url=url)
            single_time = time.time() - single_start
            print(f"   ğŸ“ {url}: {single_time:.2f}s (å…§å®¹: {len(result.markdown)} å­—å…ƒ)")
    
    total_time = time.time() - start_time
    print(f"â±ï¸ ç¸½åŸ·è¡Œæ™‚é–“: {total_time:.2f}s")
    print(f"ğŸ“ˆ å¹³å‡æ¯å€‹URL: {total_time/len(urls):.2f}s")

async def main():
    """ä¸»å‡½æ•¸"""
    print("=" * 60)
    print("ğŸ§ª Crawl4AI CLI æ¸¬è©¦ç¨‹å¼")
    print("=" * 60)
    
    try:
        await json_extraction_test()
        await regex_chunking_test()
        await links_analysis_test()
        await metadata_extraction_test()
        await performance_test()
        
        print("\n" + "=" * 60)
        print("ğŸ‰ æ‰€æœ‰ CLI æ¸¬è©¦å®Œæˆï¼")
        print("=" * 60)
        
    except Exception as e:
        print(f"âŒ æ¸¬è©¦éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    asyncio.run(main())
