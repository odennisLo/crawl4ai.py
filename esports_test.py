#!/usr/bin/env python3
"""
é›»ç«¶æ–°èç¶²ç«™çˆ¬å–æ¸¬è©¦ - å°ˆé–€é‡å° esports.net
"""

import asyncio
import json
import re
import os
import aiohttp
import aiofiles
import ssl
from datetime import datetime
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, BrowserConfig

async def download_image(session, url, file_path, semaphore):
    """ä¸‹è¼‰å–®å¼µåœ–ç‰‡"""
    async with semaphore:
        try:
            async with session.get(url) as response:
                if response.status == 200:
                    content = await response.read()
                    async with aiofiles.open(file_path, 'wb') as f:
                        await f.write(content)
                    return {"url": url, "file_path": file_path, "status": "success", "size": len(content)}
                else:
                    return {"url": url, "file_path": file_path, "status": "failed", "error": f"HTTP {response.status}"}
        except Exception as e:
            return {"url": url, "file_path": file_path, "status": "failed", "error": str(e)}

async def download_images_batch(images, base_folder, test_name):
    """æ‰¹é‡ä¸‹è¼‰åœ–ç‰‡"""
    # å»ºç«‹è³‡æ–™å¤¾åç¨±ï¼šæ¸¬è©¦åç¨±+image+åŸ·è¡Œæ™‚é–“
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    folder_name = f"{test_name}_images_{timestamp}"
    images_folder = os.path.join(base_folder, folder_name)
    
    # å»ºç«‹è³‡æ–™å¤¾
    os.makedirs(images_folder, exist_ok=True)
    print(f"ğŸ“ å»ºç«‹åœ–ç‰‡è³‡æ–™å¤¾: {folder_name}")
    
    # å»ºç«‹ HTTP session å’Œ semaphoreï¼ˆé™åˆ¶ä½µç™¼ï¼‰
    semaphore = asyncio.Semaphore(5)  # æœ€å¤šåŒæ™‚ä¸‹è¼‰5å¼µåœ–ç‰‡
    
    # å»ºç«‹ SSL é…ç½®ï¼ˆè·³éè­‰æ›¸é©—è­‰ï¼‰
    ssl_context = ssl.create_default_context()
    ssl_context.check_hostname = False
    ssl_context.verify_mode = ssl.CERT_NONE
    
    connector = aiohttp.TCPConnector(limit=10, ssl=ssl_context)
    timeout = aiohttp.ClientTimeout(total=30)
    
    download_results = []
    
    async with aiohttp.ClientSession(connector=connector, timeout=timeout) as session:
        tasks = []
        
        for i, img in enumerate(images, 1):
            img_url = img.get('src', '')
            if not img_url or img_url.startswith('data:'):
                continue
            
            # å¾ URL ç²å–æª”æ¡ˆå‰¯æª”å
            try:
                file_extension = img_url.split('.')[-1].split('?')[0].lower()
                if file_extension not in ['jpg', 'jpeg', 'png', 'gif', 'webp', 'svg']:
                    file_extension = 'jpg'  # é è¨­ç‚º jpg
            except:
                file_extension = 'jpg'
            
            # å»ºç«‹æª”æ¡ˆåç¨±
            img_alt = img.get('alt', '').replace('/', '_').replace('\\', '_')[:50]  # é™åˆ¶é•·åº¦ä¸¦ç§»é™¤ç‰¹æ®Šå­—å…ƒ
            filename = f"img_{i:03d}_{img_alt}_{timestamp}.{file_extension}"
            filename = "".join(c for c in filename if c.isalnum() or c in '._-')  # åªä¿ç•™å®‰å…¨å­—å…ƒ
            
            file_path = os.path.join(images_folder, filename)
            
            # å»ºç«‹ä¸‹è¼‰ä»»å‹™
            task = download_image(session, img_url, file_path, semaphore)
            tasks.append((task, img_url, filename, img))
        
        print(f"ğŸš€ é–‹å§‹ä¸‹è¼‰ {len(tasks)} å¼µåœ–ç‰‡...")
        
        # åŸ·è¡Œæ‰€æœ‰ä¸‹è¼‰ä»»å‹™
        for i, (task, url, filename, img_info) in enumerate(tasks, 1):
            try:
                result = await task
                download_results.append({
                    'index': i,
                    'filename': filename,
                    'url': url,
                    'alt': img_info.get('alt', ''),
                    'success': result['status'] == 'success',
                    'error': result.get('error', ''),
                    'size': result.get('size', 0)
                })
                
                if result['status'] == 'success':
                    print(f"   âœ… ({i}/{len(tasks)}) {filename} - {result.get('size', 0)} bytes")
                else:
                    print(f"   âŒ ({i}/{len(tasks)}) {filename} - {result.get('error', '')}")
                    
            except Exception as e:
                print(f"   ğŸ’¥ ({i}/{len(tasks)}) {filename} - ä¸‹è¼‰ç•°å¸¸: {str(e)}")
                download_results.append({
                    'index': i,
                    'filename': filename,
                    'url': url,
                    'alt': img_info.get('alt', ''),
                    'success': False,
                    'error': str(e),
                    'size': 0
                })
    
    # çµ±è¨ˆçµæœ
    successful_downloads = sum(1 for r in download_results if r['success'])
    print(f"\nğŸ“Š ä¸‹è¼‰å®Œæˆçµ±è¨ˆ:")
    print(f"   âœ… æˆåŠŸ: {successful_downloads} å¼µ")
    print(f"   âŒ å¤±æ•—: {len(download_results) - successful_downloads} å¼µ")
    print(f"   ğŸ“ å„²å­˜ä½ç½®: {images_folder}")
    
    # å„²å­˜ä¸‹è¼‰å ±å‘Šï¼ˆä½¿ç”¨ç›¸å°è·¯å¾‘ï¼‰
    report_path = os.path.join(images_folder, "download_report.json")
    # å–å¾—ç›¸å°æ–¼ç•¶å‰å·¥ä½œç›®éŒ„çš„è·¯å¾‘
    relative_folder_path = os.path.relpath(images_folder)
    relative_report_path = os.path.relpath(report_path)
    
    async with aiofiles.open(report_path, 'w', encoding='utf-8') as f:
        await f.write(json.dumps({
            'test_name': test_name,
            'timestamp': timestamp,
            'folder_path': relative_folder_path,  # ä½¿ç”¨ç›¸å°è·¯å¾‘
            'total_images': len(download_results),
            'successful_downloads': successful_downloads,
            'failed_downloads': len(download_results) - successful_downloads,
            'download_results': download_results
        }, indent=2, ensure_ascii=False))
    
    print(f"   ğŸ“„ ä¸‹è¼‰å ±å‘Š: {relative_report_path}")
    
    return images_folder, download_results

async def esports_news_test():
    """é›»ç«¶æ–°èæ–‡ç« å’Œåœ–ç‰‡æ“·å–æ¸¬è©¦"""
    print("ğŸ® é–‹å§‹é›»ç«¶æ–°èç¶²ç«™æ¸¬è©¦...")
    
    url = "https://www.esports.net/news/counter-strike/cs2-roster-shake-up-heroic-benches-gr1ks-fut-signs-ex-navi-juniors-with-misutaaa/"
    
    # è¨­å®šçˆ¬èŸ²åƒæ•¸
    crawler_config = CrawlerRunConfig(
        word_count_threshold=10,
        excluded_tags=[
            "script",
            "footer", 
            "link",
            "iframe"
        ],
        excluded_selector="[class^='_mentions'],[class='container__bannerZone'],[class^='_topBanner'],[class^='_wallpaperBanner'],[id^='lsadvert'],[class^='_newsSection']",
        exclude_external_links=False,
        screenshot=True,  # é–‹å•Ÿæˆªåœ–åŠŸèƒ½
    )
    
    # è¨­å®šç€è¦½å™¨åƒæ•¸
    browser_config = BrowserConfig(
        headless=True
    )
    
    async with AsyncWebCrawler(config=browser_config) as crawler:
        print(f"ğŸ“° æ­£åœ¨çˆ¬å–é›»ç«¶æ–°è: {url}")
        
        result = await crawler.arun(
            url=url,
            config=crawler_config
        )
        
        print(f"âœ… çˆ¬å–å®Œæˆï¼")
        print(f"ğŸ“„ é é¢æ¨™é¡Œ: {result.metadata.get('title', 'N/A')}")
        print(f"ğŸ”— URL: {result.url}")
        print(f"ğŸ“ å…§å®¹é•·åº¦: {len(result.markdown)} å­—å…ƒ")
        
        # é¡¯ç¤ºæ–‡ç« å…§å®¹é è¦½
        if result.markdown:
            print(f"\nğŸ“– æ–‡ç« å…§å®¹é è¦½:")
            print("-" * 60)
            preview = result.markdown[:800] + "..." if len(result.markdown) > 800 else result.markdown
            print(preview)
            print("-" * 60)
        
        # è™•ç†åœ–ç‰‡ - ä½¿ç”¨å…§å»ºçš„ media å±¬æ€§
        print(f"\nğŸ–¼ï¸ å…§å»ºåœ–ç‰‡æ“·å–çµæœ:")
        if hasattr(result, 'media') and result.media and 'images' in result.media:
            images = result.media['images']
            print(f"   æ‰¾åˆ° {len(images)} å¼µåœ–ç‰‡")
            
            # é¡¯ç¤ºå‰10å¼µåœ–ç‰‡è³‡è¨Š
            for i, img in enumerate(images[:10], 1):
                print(f"   ğŸ“· åœ–ç‰‡ {i}: {img.get('src', 'N/A')}")
                print(f"       Alt: {img.get('alt', 'N/A')}")
                print(f"       å°ºå¯¸: {img.get('width', 'N/A')} x {img.get('height', 'N/A')}")
                print()
            
            # ä¸‹è¼‰æ‰€æœ‰åœ–ç‰‡
            print(f"\nğŸ“¥ é–‹å§‹ä¸‹è¼‰åœ–ç‰‡...")
            images_folder, download_results = await download_images_batch(
                images, 
                ".", 
                "esports_news"
            )
            
        else:
            print(f"   âŒ å…§å»ºåœ–ç‰‡æ“·å–æœªæ‰¾åˆ°åœ–ç‰‡")
            print(f"   Media å±¬æ€§: {hasattr(result, 'media')}")
            if hasattr(result, 'media'):
                print(f"   Media å…§å®¹: {result.media}")
        
        # è™•ç†åœ–ç‰‡ - å¾ HTML å…§å®¹ä¸­æ‰‹å‹•è§£æ
        print(f"\nğŸ” æ‰‹å‹•è§£æ HTML ä¸­çš„åœ–ç‰‡:")
        if result.cleaned_html:
            import re
            # ä½¿ç”¨æ­£è¦è¡¨é”å¼æ‰¾å‡ºæ‰€æœ‰ img æ¨™ç±¤
            img_pattern = r'<img[^>]*src=["\'](.*?)["\'][^>]*alt=["\'](.*?)["\'][^>]*>'
            img_matches = re.findall(img_pattern, result.cleaned_html, re.IGNORECASE)
            
            print(f"   æ‰¾åˆ° {len(img_matches)} å€‹åœ–ç‰‡æ¨™ç±¤")
            for i, (src, alt) in enumerate(img_matches[:5], 1):
                print(f"   ğŸ“· HTMLåœ–ç‰‡ {i}:")
                print(f"       ä¾†æº: {src}")
                print(f"       èªªæ˜: {alt}")
                print()
        
        # è™•ç†æ›´è¤‡é›œçš„ img æ¨™ç±¤è§£æ
        if result.cleaned_html:
            # æ›´å®Œæ•´çš„åœ–ç‰‡è§£æ
            img_complex_pattern = r'<img[^>]*>'
            complex_matches = re.findall(img_complex_pattern, result.cleaned_html, re.IGNORECASE)
            print(f"   ğŸ” ç¸½å…±æ‰¾åˆ° {len(complex_matches)} å€‹ img æ¨™ç±¤")
            
            # è§£ææ¯å€‹ img æ¨™ç±¤çš„å±¬æ€§
            for i, img_tag in enumerate(complex_matches[:5], 1):
                src_match = re.search(r'src=["\'](.*?)["\']', img_tag, re.IGNORECASE)
                alt_match = re.search(r'alt=["\'](.*?)["\']', img_tag, re.IGNORECASE)
                class_match = re.search(r'class=["\'](.*?)["\']', img_tag, re.IGNORECASE)
                
                print(f"   ğŸ“· è¤‡é›œè§£æ {i}:")
                print(f"       æ¨™ç±¤: {img_tag[:100]}...")
                print(f"       ä¾†æº: {src_match.group(1) if src_match else 'N/A'}")
                print(f"       èªªæ˜: {alt_match.group(1) if alt_match else 'N/A'}")
                print(f"       é¡åˆ¥: {class_match.group(1) if class_match else 'N/A'}")
                print()
        
        # å„²å­˜æˆªåœ–
        if result.screenshot:
            screenshot_path = "esports_screenshot.png"
            if isinstance(result.screenshot, str):
                # å¦‚æœæ˜¯ base64 å­—ä¸²ï¼Œéœ€è¦è§£ç¢¼
                import base64
                screenshot_data = base64.b64decode(result.screenshot)
            else:
                # å¦‚æœå·²ç¶“æ˜¯ bytes
                screenshot_data = result.screenshot
                
            with open(screenshot_path, "wb") as f:
                f.write(screenshot_data)
            print(f"ğŸ“¸ é é¢æˆªåœ–å·²å„²å­˜è‡³: esports_screenshot.png")
        
        # åˆ†æé€£çµ
        if result.links:
            internal_links = result.links.get('internal', [])
            external_links = result.links.get('external', [])
            
            print(f"\nğŸ”— é€£çµåˆ†æ:")
            print(f"   ğŸ  å…§éƒ¨é€£çµ: {len(internal_links)} å€‹")
            print(f"   ğŸŒ å¤–éƒ¨é€£çµ: {len(external_links)} å€‹")
            
            if internal_links:
                print(f"\nğŸ“‹ ç›¸é—œæ–‡ç« é€£çµ (å‰5å€‹):")
                for i, link in enumerate(internal_links[:5], 1):
                    print(f"   {i}. {link}")
        
        # å„²å­˜å®Œæ•´çµæœåˆ°æª”æ¡ˆ
        result_data = {
            'title': result.metadata.get('title', ''),
            'url': result.url,
            'content_length': len(result.markdown),
            'images_count': len(result.media.get('images', [])) if hasattr(result, 'media') and result.media else 0,
            'internal_links_count': len(result.links.get('internal', [])) if result.links else 0,
            'external_links_count': len(result.links.get('external', [])) if result.links else 0,
            'content_preview': result.markdown[:1000] if result.markdown else '',
            'timestamp': '2025-08-14'
        }
        
        with open("esports_result.json", "w", encoding="utf-8") as f:
            json.dump(result_data, f, indent=2, ensure_ascii=False)
        
        print(f"\nğŸ’¾ å®Œæ•´çµæœå·²å„²å­˜è‡³: esports_result.json")

async def esports_with_css_selector_test():
    """ä½¿ç”¨ CSS é¸æ“‡å™¨å°ˆé–€æ“·å–æ–‡ç« å…§å®¹"""
    print("\nğŸ¯ ä½¿ç”¨ CSS é¸æ“‡å™¨æ¸¬è©¦...")
    
    url = "https://www.esports.net/news/counter-strike/cs2-roster-shake-up-heroic-benches-gr1ks-fut-signs-ex-navi-juniors-with-misutaaa/"
    
    # é‡å° esports.net çš„æ–‡ç« é¸æ“‡å™¨
    css_selectors = [
        "article .content",  # æ–‡ç« å…§å®¹
        ".article-content",  # å¯èƒ½çš„æ–‡ç« å®¹å™¨
        "[class*='article']", # åŒ…å« article çš„ class
        ".post-content",     # å¯èƒ½çš„æ–‡ç« å…§å®¹
        "main article"       # ä¸»è¦æ–‡ç« å€åŸŸ
    ]
    
    crawler_config = CrawlerRunConfig(
        word_count_threshold=10,
        excluded_tags=["script", "footer", "link", "iframe"],
        excluded_selector="[class^='_mentions'],[class='container__bannerZone'],[class^='_topBanner'],[class^='_wallpaperBanner'],[id^='lsadvert'],[class^='_newsSection']",
        exclude_external_links=False
    )
    
    browser_config = BrowserConfig(headless=True)
    
    async with AsyncWebCrawler(config=browser_config) as crawler:
        for i, selector in enumerate(css_selectors, 1):
            try:
                print(f"ğŸ” æ¸¬è©¦é¸æ“‡å™¨ {i}: {selector}")
                
                config_with_selector = CrawlerRunConfig(
                    word_count_threshold=10,
                    excluded_tags=["script", "footer", "link", "iframe"],
                    excluded_selector="[class^='_mentions'],[class='container__bannerZone'],[class^='_topBanner'],[class^='_wallpaperBanner'],[id^='lsadvert'],[class^='_newsSection']",
                    exclude_external_links=False,
                    css_selector=selector
                )
                
                result = await crawler.arun(
                    url=url,
                    config=config_with_selector
                )
                
                if result.markdown and len(result.markdown) > 100:
                    print(f"   âœ… æˆåŠŸæ“·å– {len(result.markdown)} å­—å…ƒ")
                    preview = result.markdown[:200] + "..." if len(result.markdown) > 200 else result.markdown
                    print(f"   ğŸ“– å…§å®¹: {preview}")
                    print()
                else:
                    print(f"   âŒ æ“·å–å…§å®¹éå°‘æˆ–å¤±æ•—")
                    print()
                    
            except Exception as e:
                print(f"   âŒ é¸æ“‡å™¨å¤±æ•—: {str(e)}")
                print()

async def esports_images_extraction_test():
    """å°ˆé–€æ“·å–åœ–ç‰‡çš„æ¸¬è©¦"""
    print("\nğŸ–¼ï¸ å°ˆé–€åœ–ç‰‡æ“·å–æ¸¬è©¦...")
    
    url = "https://www.esports.net/news/counter-strike/cs2-roster-shake-up-heroic-benches-gr1ks-fut-signs-ex-navi-juniors-with-misutaaa/"
    
    # æ”¹è‰¯çš„ JavaScript ä»£ç¢¼ä¾†æ“·å–é é¢ä¸­çš„æ‰€æœ‰åœ–ç‰‡
    js_code = """
    // ç­‰å¾…åœ–ç‰‡è¼‰å…¥
    await new Promise(resolve => {
        if (document.readyState === 'complete') {
            resolve();
        } else {
            window.addEventListener('load', resolve);
        }
    });
    
    // ç­‰å¾…é¡å¤–çš„æ™‚é–“è®“å‹•æ…‹å…§å®¹è¼‰å…¥
    await new Promise(resolve => setTimeout(resolve, 2000));
    
    // ç²å–æ‰€æœ‰åœ–ç‰‡ï¼ŒåŒ…æ‹¬ srcset ä¸­çš„åœ–ç‰‡
    const images = [];
    const imgElements = document.querySelectorAll('img');
    
    imgElements.forEach((img, index) => {
        const imageData = {
            index: index + 1,
            src: img.src || '',
            alt: img.alt || '',
            width: img.naturalWidth || img.width || 'N/A',
            height: img.naturalHeight || img.height || 'N/A',
            className: img.className || '',
            id: img.id || '',
            title: img.title || '',
            srcset: img.srcset || '',
            loading: img.loading || '',
            decoding: img.decoding || '',
            fetchpriority: img.getAttribute('fetchpriority') || ''
        };
        
        // ä¸éæ¿¾ä»»ä½•åœ–ç‰‡ï¼ŒåŒ…æ‹¬ data: é–‹é ­çš„
        images.push(imageData);
        
        // å¦‚æœæœ‰ srcsetï¼Œä¹Ÿæå–å…¶ä¸­çš„ URL
        if (img.srcset) {
            const srcsetUrls = img.srcset.split(',').map(entry => {
                const parts = entry.trim().split(' ');
                return {
                    url: parts[0],
                    width: parts[1] || 'N/A'
                };
            });
            imageData.srcset_urls = srcsetUrls;
        }
    });
    
    // ä¹ŸæŸ¥æ‰¾èƒŒæ™¯åœ–ç‰‡
    const elementsWithBackground = document.querySelectorAll('*');
    const backgroundImages = [];
    
    elementsWithBackground.forEach((el, index) => {
        const style = window.getComputedStyle(el);
        const backgroundImage = style.backgroundImage;
        
        if (backgroundImage && backgroundImage !== 'none') {
            const urlMatch = backgroundImage.match(/url\\(["']?([^"')]+)["']?\\)/);
            if (urlMatch) {
                backgroundImages.push({
                    index: index + 1,
                    element: el.tagName.toLowerCase(),
                    className: el.className,
                    backgroundImage: urlMatch[1]
                });
            }
        }
    });
    
    const result = {
        page_title: document.title,
        page_url: window.location.href,
        total_images: images.length,
        total_background_images: backgroundImages.length,
        images: images,
        background_images: backgroundImages,
        document_ready_state: document.readyState,
        timestamp: new Date().toISOString()
    };
    
    return result;
    """
    
    crawler_config = CrawlerRunConfig(
        word_count_threshold=10,
        excluded_tags=["script", "footer", "link"],  # ç§»é™¤ iframeï¼Œå› ç‚ºå¯èƒ½åŒ…å«åœ–ç‰‡
        excluded_selector="[class^='_mentions'],[class='container__bannerZone'],[class^='_topBanner'],[class^='_wallpaperBanner'],[id^='lsadvert'],[class^='_newsSection']",
        exclude_external_links=False,
        js_code=js_code,
        wait_for="css:img"  # ç­‰å¾…åœ–ç‰‡å…ƒç´ è¼‰å…¥
    )
    
    browser_config = BrowserConfig(headless=True)
    
    async with AsyncWebCrawler(config=browser_config) as crawler:
        result = await crawler.arun(
            url=url,
            config=crawler_config
        )
        
        print(f"âœ… JavaScript åœ–ç‰‡æ“·å–å®Œæˆ")
        
        if result.js_execution_result:
            js_result = result.js_execution_result
            
            # è™•ç† Crawl4AI çš„çµæœæ ¼å¼
            if isinstance(js_result, dict) and 'results' in js_result and js_result['results']:
                actual_result = js_result['results'][0]
            else:
                actual_result = js_result
            
            print(f"ğŸ–¼ï¸ æ‰¾åˆ° {actual_result.get('total_images', 0)} å¼µåœ–ç‰‡")
            print(f"ğŸ¨ æ‰¾åˆ° {actual_result.get('total_background_images', 0)} å¼µèƒŒæ™¯åœ–ç‰‡")
            print(f"ğŸ“„ é é¢ç‹€æ…‹: {actual_result.get('document_ready_state', 'N/A')}")
            
            images = actual_result.get('images', [])
            for i, img in enumerate(images[:10], 1):  # é¡¯ç¤ºå‰10å¼µ
                print(f"   ğŸ“· åœ–ç‰‡ {i}:")
                print(f"       ä¾†æº: {img.get('src', 'N/A')}")
                print(f"       èªªæ˜: {img.get('alt', 'N/A')}")
                print(f"       å°ºå¯¸: {img.get('width', 'N/A')} x {img.get('height', 'N/A')}")
                print(f"       CSSé¡åˆ¥: {img.get('className', 'N/A')}")
                if img.get('srcset'):
                    print(f"       Srcset: {img.get('srcset', 'N/A')[:100]}...")
                print()
            
            # ä¸‹è¼‰ JavaScript æ“·å–çš„åœ–ç‰‡
            if images:
                print(f"\nğŸ“¥ ä¸‹è¼‰ JavaScript æ“·å–çš„åœ–ç‰‡...")
                js_images_folder, js_download_results = await download_images_batch(
                    images, 
                    ".", 
                    "esports_js"
                )
            
            # é¡¯ç¤ºèƒŒæ™¯åœ–ç‰‡
            bg_images = actual_result.get('background_images', [])
            if bg_images:
                print(f"\nğŸ¨ èƒŒæ™¯åœ–ç‰‡:")
                for i, bg in enumerate(bg_images[:5], 1):
                    print(f"   ğŸ–¼ï¸ èƒŒæ™¯ {i}: {bg.get('backgroundImage', 'N/A')}")
            
            # å„²å­˜åœ–ç‰‡è³‡è¨Š
            with open("esports_images.json", "w", encoding="utf-8") as f:
                json.dump(actual_result, f, indent=2, ensure_ascii=False)
            print(f"\nğŸ’¾ åœ–ç‰‡è³‡è¨Šå·²å„²å­˜è‡³: esports_images.json")
        else:
            print(f"âŒ JavaScript åŸ·è¡Œå¤±æ•—æˆ–ç„¡çµæœ")
            print(f"åŸ·è¡Œçµæœ: {result.js_execution_result}")

async def main():
    """ä¸»å‡½æ•¸"""
    print("=" * 60)
    print("ğŸ® é›»ç«¶æ–°èç¶²ç«™å°ˆé–€æ¸¬è©¦")
    print("=" * 60)
    
    try:
        await esports_news_test()
        await esports_with_css_selector_test()
        await esports_images_extraction_test()
        
        print("\n" + "=" * 60)
        print("ğŸ‰ æ‰€æœ‰é›»ç«¶æ–°èæ¸¬è©¦å®Œæˆï¼")
        print("=" * 60)
        print("\nğŸ“ ç”Ÿæˆçš„æª”æ¡ˆ:")
        print("   ğŸ“„ esports_result.json - å®Œæ•´çµæœè³‡æ–™")
        print("   ğŸ–¼ï¸ esports_images.json - åœ–ç‰‡è³‡è¨Š")
        print("   ğŸ“¸ esports_screenshot.png - é é¢æˆªåœ–")
        print("   ğŸ“ images/ - åœ–ç‰‡è³‡æ–™å¤¾")
        
    except Exception as e:
        print(f"âŒ æ¸¬è©¦éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    asyncio.run(main())
