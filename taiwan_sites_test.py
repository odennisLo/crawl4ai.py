#!/usr/bin/env python3
"""
å°ç£ç¶²ç«™çˆ¬å–ç¯„ä¾‹
"""

import asyncio
import json
from crawl4ai import AsyncWebCrawler
from crawl4ai.extraction_strategy import JsonCssExtractionStrategy

async def taiwan_news_test():
    """å°ç£æ–°èç¶²ç«™æ¸¬è©¦"""
    print("ğŸ‡¹ğŸ‡¼ é–‹å§‹å°ç£æ–°èç¶²ç«™æ¸¬è©¦...")
    
    taiwan_news_sites = [
        ("ä¸­å¤®ç¤¾", "https://www.cna.com.tw"),
        ("è‡ªç”±æ™‚å ±", "https://www.ltn.com.tw"),
        ("è¯åˆæ–°èç¶²", "https://udn.com"),
    ]
    
    async with AsyncWebCrawler() as crawler:
        for name, url in taiwan_news_sites:
            try:
                print(f"ğŸ“° æ­£åœ¨çˆ¬å– {name}: {url}")
                result = await crawler.arun(url=url)
                
                print(f"   âœ… {name} çˆ¬å–æˆåŠŸ")
                print(f"   ğŸ“„ æ¨™é¡Œ: {result.metadata.get('title', 'N/A')}")
                print(f"   ğŸ“ å…§å®¹é•·åº¦: {len(result.markdown)} å­—å…ƒ")
                print(f"   ğŸ”— é€£çµæ•¸é‡: {len(result.links.get('internal', [])) + len(result.links.get('external', []))}")
                
                # é¡¯ç¤ºéƒ¨åˆ†å…§å®¹
                if result.markdown:
                    preview = result.markdown[:200] + "..." if len(result.markdown) > 200 else result.markdown
                    print(f"   ğŸ“– å…§å®¹é è¦½: {preview}")
                print()
                
            except Exception as e:
                print(f"   âŒ {name} çˆ¬å–å¤±æ•—: {str(e)}")
                print()

async def ptt_test():
    """PTT ç¶²ç«™æ¸¬è©¦"""
    print("ğŸ’¬ é–‹å§‹ PTT ç¶²ç«™æ¸¬è©¦...")
    
    async with AsyncWebCrawler() as crawler:
        try:
            print("ğŸ“ æ­£åœ¨çˆ¬å– PTT é¦–é ...")
            result = await crawler.arun(url="https://www.ptt.cc/bbs/index.html")
            
            print(f"âœ… PTT çˆ¬å–æˆåŠŸ")
            print(f"ğŸ“„ æ¨™é¡Œ: {result.metadata.get('title', 'N/A')}")
            print(f"ğŸ“ å…§å®¹é•·åº¦: {len(result.markdown)} å­—å…ƒ")
            
            if result.markdown:
                preview = result.markdown[:300] + "..." if len(result.markdown) > 300 else result.markdown
                print(f"ğŸ“– å…§å®¹é è¦½:\n{preview}")
                
        except Exception as e:
            print(f"âŒ PTT çˆ¬å–å¤±æ•—: {str(e)}")

async def government_site_test():
    """æ”¿åºœç¶²ç«™æ¸¬è©¦"""
    print("\nğŸ›ï¸ é–‹å§‹æ”¿åºœç¶²ç«™æ¸¬è©¦...")
    
    gov_sites = [
        ("ç¸½çµ±åºœ", "https://www.president.gov.tw"),
        ("è¡Œæ”¿é™¢", "https://www.ey.gov.tw"),
        ("æ•¸ä½ç™¼å±•éƒ¨", "https://www.moda.gov.tw"),
    ]
    
    async with AsyncWebCrawler() as crawler:
        for name, url in gov_sites:
            try:
                print(f"ğŸ›ï¸ æ­£åœ¨çˆ¬å– {name}: {url}")
                result = await crawler.arun(url=url)
                
                print(f"   âœ… {name} çˆ¬å–æˆåŠŸ")
                print(f"   ğŸ“„ æ¨™é¡Œ: {result.metadata.get('title', 'N/A')}")
                print(f"   ğŸ“ å…§å®¹é•·åº¦: {len(result.markdown)} å­—å…ƒ")
                print()
                
            except Exception as e:
                print(f"   âŒ {name} çˆ¬å–å¤±æ•—: {str(e)}")
                print()

async def ecommerce_test():
    """é›»å•†ç¶²ç«™æ¸¬è©¦"""
    print("ğŸ›’ é–‹å§‹é›»å•†ç¶²ç«™æ¸¬è©¦...")
    
    ecommerce_sites = [
        ("PChome", "https://www.pchome.com.tw"),
        ("momoè³¼ç‰©ç¶²", "https://www.momoshop.com.tw"),
    ]
    
    async with AsyncWebCrawler() as crawler:
        for name, url in ecommerce_sites:
            try:
                print(f"ğŸ›’ æ­£åœ¨çˆ¬å– {name}: {url}")
                result = await crawler.arun(url=url)
                
                print(f"   âœ… {name} çˆ¬å–æˆåŠŸ")
                print(f"   ğŸ“„ æ¨™é¡Œ: {result.metadata.get('title', 'N/A')}")
                print(f"   ğŸ“ å…§å®¹é•·åº¦: {len(result.markdown)} å­—å…ƒ")
                print()
                
            except Exception as e:
                print(f"   âŒ {name} çˆ¬å–å¤±æ•—: {str(e)}")
                print()

async def tech_blog_test():
    """ç§‘æŠ€éƒ¨è½æ ¼æ¸¬è©¦"""
    print("ğŸ’» é–‹å§‹ç§‘æŠ€éƒ¨è½æ ¼æ¸¬è©¦...")
    
    tech_blogs = [
        ("iThome", "https://www.ithome.com.tw"),
        ("ç§‘æŠ€æ–°å ±", "https://technews.tw"),
    ]
    
    async with AsyncWebCrawler() as crawler:
        for name, url in tech_blogs:
            try:
                print(f"ğŸ’» æ­£åœ¨çˆ¬å– {name}: {url}")
                result = await crawler.arun(url=url)
                
                print(f"   âœ… {name} çˆ¬å–æˆåŠŸ")
                print(f"   ğŸ“„ æ¨™é¡Œ: {result.metadata.get('title', 'N/A')}")
                print(f"   ğŸ“ å…§å®¹é•·åº¦: {len(result.markdown)} å­—å…ƒ")
                print()
                
            except Exception as e:
                print(f"   âŒ {name} çˆ¬å–å¤±æ•—: {str(e)}")
                print()

async def main():
    """ä¸»å‡½æ•¸"""
    print("=" * 60)
    print("ğŸ‡¹ğŸ‡¼ å°ç£ç¶²ç«™çˆ¬å–æ¸¬è©¦ç¨‹å¼")
    print("=" * 60)
    
    try:
        await taiwan_news_test()
        await ptt_test()
        await government_site_test()
        await ecommerce_test()
        await tech_blog_test()
        
        print("=" * 60)
        print("ğŸ‰ æ‰€æœ‰å°ç£ç¶²ç«™æ¸¬è©¦å®Œæˆï¼")
        print("=" * 60)
        
    except Exception as e:
        print(f"âŒ æ¸¬è©¦éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    asyncio.run(main())
